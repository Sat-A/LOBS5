Error while loading conda entry point: conda-libmamba-solver (cannot import name 'Spinner' from 'conda.common.io' (/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/lib/python3.12/site-packages/conda/common/io.py))
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
wandb: WARNING Disabling SSL verification.  Connections to this server are not verified and may be insecure!
/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.
W1113 10:36:03.679756  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:36:04.028684  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:36:04.048153  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:36:04.070887  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:36:04.092538  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
wandb: Currently logged in as: kang-oxford (zheng-xiong-University of Oxford) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5/wandb/run-20251113_103604-jqd35xzr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-forest-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kang-oxford/lobs5-full-autoreg
wandb: üöÄ View run at https://wandb.ai/kang-oxford/lobs5-full-autoreg/runs/jqd35xzr
1113 10:37:37.364939  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.

  0%|          | 0/32091 [00:00<?, ?it/s]ERROR:2025-11-13 10:37:42,509:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()
Traceback (most recent call last):
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax/_src/xla_bridge.py", line 485, in discover_pjrt_plugins
    plugin_module.initialize()
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 328, in initialize
    _check_cuda_versions(raise_on_first_error=True)
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 285, in _check_cuda_versions
    local_device_count = cuda_versions.cuda_device_count()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: CUDA_ERROR_NO_DEVICE
ERROR:2025-11-13 10:37:42,509:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()
Traceback (most recent call last):
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax/_src/xla_bridge.py", line 485, in discover_pjrt_plugins
    plugin_module.initialize()
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 328, in initialize
    _check_cuda_versions(raise_on_first_error=True)
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 285, in _check_cuda_versions
    local_device_count = cuda_versions.cuda_device_count()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: CUDA_ERROR_NO_DEVICE
ERROR:2025-11-13 10:37:42,513:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()
Traceback (most recent call last):
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax/_src/xla_bridge.py", line 485, in discover_pjrt_plugins
    plugin_module.initialize()
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 328, in initialize
    _check_cuda_versions(raise_on_first_error=True)
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 285, in _check_cuda_versions
    local_device_count = cuda_versions.cuda_device_count()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: CUDA_ERROR_NO_DEVICE
ERROR:2025-11-13 10:37:42,512:jax._src.xla_bridge:487: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()
Traceback (most recent call last):
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax/_src/xla_bridge.py", line 485, in discover_pjrt_plugins
    plugin_module.initialize()
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 328, in initialize
    _check_cuda_versions(raise_on_first_error=True)
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py", line 285, in _check_cuda_versions
    local_device_count = cuda_versions.cuda_device_count()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: CUDA_ERROR_NO_DEVICE
W1113 10:37:45.077728  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:37:45.116446  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:37:45.162104  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
W1113 10:37:45.427851  102490 sol_gpu_cost_model.cc:102] No SoL config found for device: NVIDIA GH200 120GB. Using default config.
/lus/lfs1aip2/home/s5e/kangli.s5e/miniforge3/envs/lobs5/lib/python3.11/site-packages/jax/_src/lax/lax.py:5473: ComplexWarning: Casting complex values to real discards the imaginary part
  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)
113 10:38:36.140759  102490 hlo_rematerialization.cc:3198] Can't reduce memory use below 80.45GiB (86378246501 bytes) by rematerialization; only reduced to 91.16GiB (97880554992 bytes), down from 128.49GiB (137966920360 bytes) originally
W1113 10:38:56.478578  103348 bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 93.25GiB (rounded to 100132115968)requested by op 
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
W1113 10:38:56.479099  103348 bfc_allocator.cc:512] ***_________________________________________________________________________________________________
E1113 10:38:56.479194  103348 pjrt_stream_executor_client.cc:3314] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 100132115784 bytes. [tf-allocator-allocation-error='']
W1113 10:38:56.480211  103351 bfc_allocator.cc:501] Allocator (GPU_1_bfc) ran out of memory trying to allocate 93.25GiB (rounded to 100132115968)requested by op 
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
W1113 10:38:56.480602  103351 bfc_allocator.cc:512] ***_________________________________________________________________________________________________
E1113 10:38:56.480705  103351 pjrt_stream_executor_client.cc:3314] Execution of replica 1 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 100132115784 bytes. [tf-allocator-allocation-error='']
W1113 10:38:56.481487  103354 bfc_allocator.cc:501] Allocator (GPU_2_bfc) ran out of memory trying to allocate 93.25GiB (rounded to 100132115968)requested by op 
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
W1113 10:38:56.481854  103354 bfc_allocator.cc:512] ***_________________________________________________________________________________________________
E1113 10:38:56.481939  103354 pjrt_stream_executor_client.cc:3314] Execution of replica 2 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 100132115784 bytes. [tf-allocator-allocation-error='']
W1113 10:38:56.482829  103357 bfc_allocator.cc:501] Allocator (GPU_3_bfc) ran out of memory trying to allocate 93.25GiB (rounded to 100132115968)requested by op 
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
W1113 10:38:56.483194  103357 bfc_allocator.cc:512] ***_________________________________________________________________________________________________
E1113 10:38:56.483277  103357 pjrt_stream_executor_client.cc:3314] Execution of replica 3 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 100132115784 bytes. [tf-allocator-allocation-error='']

  0%|          | 0/32091 [01:19<?, ?it/s]
Traceback (most recent call last):
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5/run_train.py", line 198, in <module>
    train(args)
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5/lob/train.py", line 198, in train
    state, train_loss,ce_by_tok ,step = train_epoch(state,
                                        ^^^^^^^^^^^^^^^^^^
  File "/lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5/lob/train_helpers.py", line 560, in train_epoch
    state, loss, ce, logits = train_step(
                              ^^^^^^^^^^^
jaxlib._jax.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 100132115784 bytes.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.
