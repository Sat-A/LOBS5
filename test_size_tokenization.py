#!/usr/bin/env python3
"""
Test script for base-100 size tokenization refactoring
Tests:
1. Vocabulary size reduction (12,012 -> 2,112)
2. Message length increase (22 -> 24 tokens)
3. Encoding/decoding round-trip correctness
"""

import numpy as np
import jax.numpy as jnp
from lob.encoding import Vocab, Message_Tokenizer, encode_msg, decode_msg

def test_vocabulary_size():
    """Test that vocabulary size is reduced to 2,112"""
    v = Vocab()
    vocab_size = len(v)
    print(f"âœ“ Vocabulary size: {vocab_size}")
    assert vocab_size == 2112, f"Expected vocabulary size 2112, got {vocab_size}"
    print("  â””â”€ Reduction from 12,012 to 2,112 tokens (82.4% reduction)")
    return v

def test_message_length():
    """Test that message length is now 24 tokens"""
    msg_len = Message_Tokenizer.MSG_LEN
    print(f"âœ“ Message length: {msg_len} tokens")
    assert msg_len == 24, f"Expected message length 24, got {msg_len}"
    print("  â””â”€ Increased from 22 to 24 tokens (9.1% increase)")

    # Check TOK_LENS
    expected_tok_lens = np.array([1, 1, 2, 2, 1, 3, 2, 3, 2, 2, 2, 3])
    actual_tok_lens = Message_Tokenizer.TOK_LENS
    assert np.array_equal(actual_tok_lens, expected_tok_lens), \
        f"TOK_LENS mismatch: expected {expected_tok_lens}, got {actual_tok_lens}"
    print(f"  â””â”€ TOK_LENS: {list(actual_tok_lens)}")
    return msg_len

def test_size_encoding_decoding(vocab):
    """Test encoding and decoding of size values"""
    print("âœ“ Testing size encoding/decoding:")

    # Test cases: edge values and common values
    test_sizes = [0, 1, 99, 100, 101, 999, 1000, 5555, 9999]

    for size_val in test_sizes:
        # Create a dummy message with the test size
        msg = np.array([
            0,      # order_id (ignored)
            1,      # event_type
            1,      # direction
            100,    # price_abs (ignored)
            100,    # price
            size_val,  # size (testing this!)
            0,      # delta_t_s
            0,      # delta_t_ns
            50000,  # time_s
            123456789,  # time_ns
            50,     # p_ref
            200,    # size_ref
            50000,  # time_s_ref
            123456789  # time_ns_ref
        ])

        # Encode
        encoded = encode_msg(jnp.array(msg), vocab.ENCODING)

        # Check encoded length
        assert len(encoded) == 24, f"Encoded message length is {len(encoded)}, expected 24"

        # Decode
        decoded = decode_msg(encoded, vocab.ENCODING)

        # Check size is correctly encoded/decoded
        decoded_size = int(decoded[5])  # size is at index 5 in the decoded message

        # Handle special NA values
        if decoded_size == -9999:  # NA_VAL
            print(f"  â””â”€ Size {size_val}: encoded as NA (special value)")
        else:
            assert decoded_size == size_val, \
                f"Size round-trip failed: {size_val} -> {decoded_size}"

            # Check the actual token values for size
            size_high_token = int(encoded[4])
            size_low_token = int(encoded[5])
            expected_high = size_val // 100
            expected_low = size_val % 100

            print(f"  â””â”€ Size {size_val:4d} â†’ tokens [{size_high_token:3d}, {size_low_token:3d}] "
                  f"(high={expected_high:2d}, low={expected_low:2d}) âœ“")

    # Test size_ref encoding
    print("\nâœ“ Testing size_ref encoding/decoding:")
    test_size_ref = 7777
    msg[11] = test_size_ref  # size_ref field

    encoded = encode_msg(jnp.array(msg), vocab.ENCODING)
    decoded = decode_msg(encoded, vocab.ENCODING)
    decoded_size_ref = int(decoded[11])

    assert decoded_size_ref == test_size_ref, \
        f"Size_ref round-trip failed: {test_size_ref} -> {decoded_size_ref}"

    size_ref_high_token = int(encoded[17])
    size_ref_low_token = int(encoded[18])
    print(f"  â””â”€ Size_ref {test_size_ref} â†’ tokens [{size_ref_high_token}, {size_ref_low_token}] âœ“")

def test_field_positions():
    """Test that field positions are correctly updated"""
    print("âœ“ Testing field positions:")

    # Check TOK_DELIM (cumulative sum of token lengths)
    expected_delim = np.cumsum(Message_Tokenizer.TOK_LENS[:-1])
    actual_delim = Message_Tokenizer.TOK_DELIM

    assert np.array_equal(actual_delim, expected_delim), \
        f"TOK_DELIM mismatch: expected {expected_delim}, got {actual_delim}"

    print(f"  â””â”€ Token boundaries: {list(actual_delim)}")

    # Test field extraction
    mt = Message_Tokenizer()

    # Size field should now be at indices 4-5 (2 tokens)
    size_field_indices = [4, 5]
    for idx in size_field_indices:
        field = Message_Tokenizer.get_field_from_idx(idx)
        assert field[0] == 'size', f"Index {idx} should map to 'size', got {field[0]}"

    # Size_ref field should now be at indices 17-18 (2 tokens)
    size_ref_field_indices = [17, 18]
    for idx in size_ref_field_indices:
        field = Message_Tokenizer.get_field_from_idx(idx)
        assert field[0] == 'size_ref', f"Index {idx} should map to 'size_ref', got {field[0]}"

    print("  â””â”€ Field mappings verified âœ“")

def main():
    print("=" * 60)
    print("Testing Base-100 Size Tokenization Refactoring")
    print("=" * 60)

    # Test 1: Vocabulary size
    vocab = test_vocabulary_size()
    print()

    # Test 2: Message length
    msg_len = test_message_length()
    print()

    # Test 3: Encoding/decoding
    test_size_encoding_decoding(vocab)
    print()

    # Test 4: Field positions
    test_field_positions()

    print("\n" + "=" * 60)
    print("âœ… All tests passed successfully!")
    print("=" * 60)

    # Summary
    print("\nðŸ“Š Summary:")
    print(f"  â€¢ Vocabulary: 12,012 â†’ 2,112 tokens (-82.4%)")
    print(f"  â€¢ Message length: 22 â†’ 24 tokens (+9.1%)")
    print(f"  â€¢ Memory saved in embedding layer: ~10.1M parameters")
    print(f"  â€¢ Size encoding: value â†’ [high_2_digits, low_2_digits]")
    print(f"    Example: 1234 â†’ [12, 34]")
    print(f"    Example: 9999 â†’ [99, 99]")

if __name__ == "__main__":
    main()