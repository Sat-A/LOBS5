#!/bin/bash
#SBATCH --job-name=lobs5_es_real
#SBATCH --output=logs_es_real/lobs5_es_%j.out
#SBATCH --error=logs_es_real/lobs5_es_%j.err
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:4
#SBATCH --mem=0
#SBATCH --time=12:00:00
#SBATCH --contiguous

# ============================================
# LOBS5 Real ES Training with Eggroll
# ============================================
# Based on binary search results: max population ~1824
# Using 1792 for safety (448 per GPU × 4)
# ============================================

echo "============================================"
echo "Job started at: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Running on nodes: $SLURM_JOB_NODELIST"
echo "GPUs: 4"
echo "============================================"
echo ""

# Create logs directory
mkdir -p logs_es_real

# Change to working directory
cd /lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5

# ============================================
# Launch Real ES Training
# ============================================
echo "[*] Starting REAL ES training at: $(date)"
echo "============================================"
echo ""

# Source conda
source ~/miniforge3/etc/profile.d/conda.sh
conda activate lobs5

# Load CUDA
module load cuda/12.6

# Set LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cuda_nvrtc/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cusparse/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cuda_cupti/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cufft/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/nvjitlink/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cusolver/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/nccl/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/nvshmem/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cublas/lib:$CONDA_PREFIX/lib/python3.11/site-packages/nvidia/cudnn/lib:$LD_LIBRARY_PATH

# JAX memory management
export XLA_PYTHON_CLIENT_PREALLOCATE=true
export XLA_PYTHON_CLIENT_MEM_FRACTION=0.90
export TF_GPU_ALLOCATOR=cuda_malloc_async
export TF_ENABLE_ONEDNN_OPTS=0
export TF_CPP_MIN_LOG_LEVEL=1

# CUDA cache
export CUDA_DEVICE_ORDER=PCI_BUS_ID
if [ -n "$SLURM_TMPDIR" ]; then
    export TMPDIR="$SLURM_TMPDIR"
    export CUDA_CACHE_PATH="$SLURM_TMPDIR/.nv/ComputeCache"
else
    export CUDA_CACHE_PATH="$HOME/.nv/ComputeCache"
fi
mkdir -p "$CUDA_CACHE_PATH" || true

echo "[*] Available GPUs:"
nvidia-smi --list-gpus | head -4

# ============================================
# Real ES Training Configuration
# ============================================
CHECKPOINT_PATH="checkpoints/lobs5_d1024_l12_b16_bsz13x4_seed42_jid1684154_3zf0yp50"
OUTPUT_DIR="es_checkpoints/real_$(date +%Y%m%d_%H%M%S)"

echo ""
echo "[*] ============================================"
echo "[*] Real ES Training Configuration:"
echo "[*]   Checkpoint: $CHECKPOINT_PATH"
echo "[*]   Output: $OUTPUT_DIR"
echo "[*]   Noiser: eggroll"
echo "[*]   Population: 1792 (448 per GPU × 4)"
echo "[*]   Epochs: 1000"
echo "[*]   Steps per episode: 100"
echo "[*] ============================================"
echo ""

mkdir -p "$OUTPUT_DIR"
mkdir -p logs_es_real

# ============================================
# Run Real ES Training with JaxLOB
# ============================================
python -u -B -m es_lobs5.training.es_jaxlob_train \
    --lobs5_checkpoint="${CHECKPOINT_PATH}" \
    --noiser=eggroll \
    --sigma=0.01 \
    --lr=0.001 \
    --lora_rank=4 \
    --n_threads=1792 \
    --n_epochs=1000 \
    --n_steps=100 \
    --world_msgs_per_step=10 \
    --task=sell \
    --task_size=500 \
    --tick_size=100 \
    --seed=42 \
    --output_dir="${OUTPUT_DIR}" \
    --wandb_project=lobs5-es-jaxlob \
    --wandb_entity=kang-oxford \
    2>&1 | tee logs_es_real/training_${SLURM_JOB_ID}.log

echo ""
echo "[*] ============================================"
echo "[*] Real ES Training Completed!"
echo "[*] Results saved to: $OUTPUT_DIR"
echo "[*] Total runtime: $SECONDS seconds"
echo "[*] ============================================"
