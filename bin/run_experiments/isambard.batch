#!/bin/bash
#SBATCH --job-name=lobs5_3072x32
#SBATCH --output=logs_lobs5_3072x32/lobs5_%j.out
#SBATCH --error=logs_lobs5_3072x32/lobs5_%j.err
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:4
#SBATCH --mem=0
#SBATCH --time=02:00:00
#SBATCH --contiguous

# ============================================
# LOBS5 Training Script (Single/Multi-Node)
# ============================================
# Usage:
#   Single-node (4 GPUs):   sbatch isambard.batch
#   Multi-node  (8 GPUs):    sbatch --nodes=2 --time=02:00:00 isambard.batch
# ============================================

echo "============================================"
echo "Job started at: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Running on nodes: $SLURM_JOB_NODELIST"
echo "Number of nodes: $SLURM_NNODES"
echo "GPUs per node (requested): 4"
echo "Total GPUs (expected): $((SLURM_NNODES * 4))"
echo "============================================"
echo ""

# Create logs directory
mkdir -p logs_lobs5_3072x32

# # Change to working directory
# cd /lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5

# ============================================
# Multi-Node Setup (if NNODES > 1)
# ============================================
GPUS_PER_NODE=4
NNODES=$SLURM_NNODES

if [ $NNODES -gt 1 ]; then
    echo "[*] Multi-node mode detected ($NNODES nodes)"
    echo "[*] JAX will use jax.distributed.initialize()"
    echo ""

    # Set master address for coordination
    MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n1)
    MASTER_PORT=29500

    # JAX distributed environment variables (CRITICAL!)
    export JAX_COORDINATOR_ADDRESS="$MASTER_ADDR:$MASTER_PORT"
    export MASTER_ADDR
    export MASTER_PORT

    echo "[*] Coordinator address: $JAX_COORDINATOR_ADDRESS"
    echo "[*] Master addr: $MASTER_ADDR"
    echo "[*] Master port: $MASTER_PORT"
else
    echo "[*] Single-node mode (4 GPUs on one node)"
fi
echo ""

# ============================================
# Launch Training
# ============================================
echo "[*] Starting training at: $(date)"
echo "============================================"
echo ""

# Run training
if [ $NNODES -eq 1 ]; then
    # Single-node: use wrapper script for environment setup
    bash bash /home/s5e/satyamaga.s5e/LOBS5/bin/run_experiments/satyam.sh\
        2>&1 | tee logs_lobs5_3072x32/training_${SLURM_JOB_ID}.log
else
    # Multi-node: use srun with wrapper script
    srun --nodes=$NNODES \
         --ntasks=$NNODES \
         --ntasks-per-node=1 \
         --gres=gpu:$GPUS_PER_NODE \
         --gpu-bind=map_gpu:0,1,2,3 \
         --output=logs_lobs5_3072x32/training_${SLURM_JOB_ID}_node%n.log \
         --export=ALL \
         bash /home/s5e/satyamaga.s5e/LOBS5/bin/run_experiments/satyam.sh
        #  bash /lus/lfs1aip2/home/s5e/kangli.s5e/AlphaTrade/LOBS5/bin/run_experiments/run_lobster_padded_large.sh
fi

echo ""
echo "============================================"
echo "Training completed at: $(date)"
echo "Total runtime: $SECONDS seconds"

# Calculate hours, minutes, seconds
hours=$((SECONDS / 3600))
minutes=$(((SECONDS % 3600) / 60))
seconds=$((SECONDS % 60))

echo "Total runtime: ${hours}h ${minutes}m ${seconds}s"
echo "============================================"